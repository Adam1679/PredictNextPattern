data: 
 data_dir: memmap_dataset
 max_seq_len: &seq_len 2048
 min_seq_len: 1024
 num_workers: 4
model: 
  !inc training/configs/llama_124M.yaml
  # !inc training/configs/llama_9M.yaml

validation: 
 interval: -1
 sample_n: 10000
 batch_size: 16
 # metrics: 
 #  - mse
 #  - mae

 filter_symbols: null
 filter_intervals: 0.0

test: 
 # metrics: 
 #  - mse
 #  - mae

 filter_symbols: null
 filter_intervals: 0.0

logging: 
 log_interval: 10
 wandb_enabled: True
 wandb_project: cpt
 wandb_run_name: 'test_run_124M'
 wandb_tags: ["your_tag1", "your_tag2"]

optimizer: 
 batch_size: &bsz 16
 total_steps: &steps 400000
 lr: 0.001
 min_lr: 0.0001
 weight_decay: 0.01
 warmup_steps: 1000

checkpointing: 
 dir: checkpoints
 interval: 1000
 keep_n_checkpoints: 1

distributed: 
 num_nodes: 1
 num_gpus: 1
 train_micro_batch_size_per_gpu: *bsz
 gradient_clipping: 1.0
 fp16: 
  enabled: False
 bf16: 
  enabled: True

 zero_optimization: 
  stage: 0