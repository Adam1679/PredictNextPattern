data: 
 data_dir: memmap_dataset
 max_seq_len: &seq_len 2048
 min_seq_len: 1024
model: 
 vocab_size:  1
 input_size: 5
 output_size: 5
 hidden_size: 256
 intermediate_size: 1024
 num_hidden_layers: 9
 num_attention_heads: 8
 num_key_value_heads: 8
 hidden_act: "silu"
 max_position_embeddings: *seq_len
 initializer_range: 0.02
 rms_norm_eps: 0.000001
 use_cache: True
 pad_token_id: 0
 bos_token_id: 1
 eos_token_id: 2
 pretraining_tp: 1
 tie_word_embeddings: False
 rope_theta: 10000.0
 attention_bias: False
 attention_dropout: 0.0
 mlp_bias: False

validation: 
 interval: 100
 first_n: 10000
 # metrics: 
 #  - mse
 #  - mae

 filter_symbols: null
 filter_intervals: 0.0

test: 
 # metrics: 
 #  - mse
 #  - mae

 filter_symbols: null
 filter_intervals: 0.0

wandb: 
 enabled: False
 project: null
 entity: null
 run_name: null
 tags: ["your_tag1", "your_tag2"]

optimizer: 
 batch_size: &bsz 32
 total_steps: &steps 10000
 lr: 0.001
 weight_decay: 0.01
 warmup_steps: 100

checkpointing: 
 dir: checkpoints
 interval: 1000
 keep_n_checkpoints: 1

distributed: 
 train_batch_size: *bsz
 fp16: 
  enabled: False

 zero_optimization: 
  stage: 2