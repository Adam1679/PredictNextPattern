data:
 version: 1
 data_root: memmap_dataset
 max_seq_len: &seq_len 2048
 min_seq_len: 1024
 num_workers: 8
 filter_symbols: null
 filter_intervals: 
  - '1h'
  - '30m'
  - '15m'
  - '5m'
 filter_types: null
model: 
  # !inc training/configs/llama_124M.yaml
  # !inc training/configs/llama_28M.yaml
  # !inc training/configs/llama_9M.yaml
  # !inc training/configs/llama_9M_category.yaml
  !inc training/configs/llama_28M_category.yaml
  # !inc training/configs/llama_124M_category.yaml


test: 
 filter_symbols: null
 filter_intervals: 0.0

logging: 
 log_interval: 10
 wandb_enabled: True
 wandb_project: cpt_fix2
 wandb_run_name: '28M_20k_steps_multi_interval_bsz128_data_drop'
 wandb_tags: ["your_tag1", "your_tag2"]

optimizer: 
 batch_size: &bsz 128
 total_steps: &steps 20_000
 lr: 0.001
 min_lr: 0.0001
 weight_decay: 0.01
 warmup_steps: 100

validation: 
 interval: 100
 sample_n: 20000
 batch_size: *bsz
 filter_symbols: null
 filter_intervals: ['1h']

checkpointing: 
 dir: checkpoints
 interval: 1000
 keep_n_checkpoints: 1

distributed: 
 num_nodes: 1
 num_gpus: 1
 train_batch_size: *bsz
 gradient_accumulation_steps: 1
 gradient_clipping: 1.0
 fp16: 
  enabled: False
 bf16: 
  enabled: True

 zero_optimization: 
  stage: 2