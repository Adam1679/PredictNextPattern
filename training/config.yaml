data: 
 data_root: memmap_dataset
 max_seq_len: &seq_len 2048
 min_seq_len: 1024
 num_workers: 4
 filter_symbols: null
 filter_intervals: '1h'
 filter_types: 'spot'
model: 
  # !inc training/configs/llama_124M.yaml
  !inc training/configs/llama_28M.yaml
  # !inc training/configs/llama_9M.yaml


test: 
 filter_symbols: null
 filter_intervals: 0.0

logging: 
 log_interval: 1
 wandb_enabled: True
 wandb_project: cpt
 wandb_run_name: '28M_1h_spot_bsz256'
 wandb_tags: ["your_tag1", "your_tag2"]

optimizer: 
 batch_size: &bsz 256
 total_steps: &steps 20000
 lr: 0.001
 min_lr: 0.0001
 weight_decay: 0.01
 warmup_steps: 1000

validation: 
 interval: 500
 sample_n: 20000
 batch_size: *bsz
 filter_symbols: null
 filter_intervals: 0.0

checkpointing: 
 dir: checkpoints
 interval: 1000
 keep_n_checkpoints: 1

distributed: 
 num_nodes: 1
 num_gpus: 1
 train_batch_size: *bsz
 gradient_accumulation_steps: 1
 gradient_clipping: 1.0
 fp16: 
  enabled: False
 bf16: 
  enabled: True

 zero_optimization: 
  stage: 2