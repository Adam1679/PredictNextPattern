data:
 version: 1
 data_root: memmap_dataset
 max_seq_len: &seq_len 512
 min_seq_len: 512
 num_workers: 4
 filter_symbols: null
 filter_intervals: 
  - '1h'
  - '30m'
  - '15m'
 filter_types: null
model: 
  # !inc training/configs/llama_124M.yaml
  # !inc training/configs/llama_28M.yaml
  # !inc training/configs/llama_9M.yaml
  # !inc training/configs/llama_9M_category.yaml
  # !inc training/configs/llama_28M_category.yaml
  # !inc training/configs/llama_28M_category_dropout.yaml
  # !inc training/configs/llama_124M_category.yaml
  !inc training/configs/t5_9M_category.yaml


test: 
 filter_symbols: null
 filter_intervals: 0.0

logging: 
 log_interval: 10
 wandb_enabled: True
 wandb_project: cpt_fix2
 wandb_run_name: '9M_llama_10k_steps_multi_interval_bsz256_cxt512_z_reg'
 wandb_tags: ["your_tag1", "your_tag2"]

optimizer: 
 batch_size: &bsz 256
 total_steps: &steps 10_000
 lr: 0.001
 min_lr: 0.0001
 weight_decay: 0.01
 warmup_steps: 256
 z_reg: 0.0001

validation: 
 interval: 200
 sample_n: 50000
 batch_size: *bsz
 filter_symbols: null
 filter_intervals: ['1h', '30m', '15m']

checkpointing: 
 dir: checkpoints
 interval: 1000
 keep_n_checkpoints: 1

distributed: 
 num_nodes: 1
 num_gpus: 1
 train_batch_size: *bsz
 gradient_accumulation_steps: 1
 gradient_clipping: 1.0
 fp16: 
  enabled: False
 bf16: 
  enabled: True

 zero_optimization: 
  stage: 2