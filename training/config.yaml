data: 
 data_dir: memmap_dataset
 max_seq_len: &seq_len 2048
 min_seq_len: 1024
 num_workers: 4
model: 
 vocab_size:  1
 input_size: 4
 output_size: 4
 hidden_size: 256
 intermediate_size: 1024
 num_hidden_layers: 9
 num_attention_heads: 8
 num_key_value_heads: 8
 hidden_act: "silu"
 max_position_embeddings: *seq_len
 initializer_range: 0.02
 rms_norm_eps: 0.000001
 use_cache: True
 pad_token_id: 0
 bos_token_id: 1
 eos_token_id: 2
 pretraining_tp: 1
 tie_word_embeddings: False
 rope_theta: 10000.0
 attention_bias: False
 attention_dropout: 0.0
 mlp_bias: False

validation: 
 interval: -1
 first_n: 10000
 batch_size: 16
 # metrics: 
 #  - mse
 #  - mae

 filter_symbols: null
 filter_intervals: 0.0

test: 
 # metrics: 
 #  - mse
 #  - mae

 filter_symbols: null
 filter_intervals: 0.0

logging: 
 log_interval: 10
 wandb_enabled: True
 wandb_project: cpt
 wandb_run_name: 'test_run'
 wandb_tags: ["your_tag1", "your_tag2"]

optimizer: 
 batch_size: &bsz 32
 total_steps: &steps 10000
 lr: 0.001
 weight_decay: 0.01
 warmup_steps: 100

checkpointing: 
 dir: checkpoints
 interval: 1000
 keep_n_checkpoints: 1

distributed: 
 train_micro_batch_size_per_gpu: *bsz
 gradient_clipping: 1.0
 fp16: 
  enabled: False
 bf16: 
  enabled: True

 zero_optimization: 
  stage: 0