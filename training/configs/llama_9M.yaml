vocab_size:  1
input_size: 4
output_size: 4
hidden_size: 256
intermediate_size: 1024
num_hidden_layers: 9
num_attention_heads: 8
num_key_value_heads: 8
hidden_act: "silu"
initializer_range: 0.02
rms_norm_eps: 0.000001
use_cache: True
pad_token_id: 0
bos_token_id: 1
eos_token_id: 2
pretraining_tp: 1
tie_word_embeddings: False
rope_theta: 10000.0
attention_bias: False
attention_dropout: 0.0
mlp_bias: False