vocab_size:  1
input_size: 0
output_size: 0
hidden_size: 384
intermediate_size: 1536
num_hidden_layers: 12
num_attention_heads: 12
num_key_value_heads: 12
hidden_act: "silu"
initializer_range: 0.02
rms_norm_eps: 0.000001
use_cache: False
pad_token_id: 0
bos_token_id: 1
eos_token_id: 2
pretraining_tp: 1
tie_word_embeddings: False
rope_theta: 10000.0
attention_bias: False
attention_dropout: 0.0
mlp_bias: False
output_hidden_states: False
output_attentions: False
_attn_implementation: flash_attention_2
categorical_features: 9
embedding_dims: [128, 32, 64, 64, 64, 32, 32, 64, 32] 
num_categories: [4096, 2, 8, 8, 6, 2, 2, 6, 2]