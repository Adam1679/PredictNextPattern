vocab_size: 1
input_size: 4
output_size: 4
hidden_size: 768
intermediate_size: 3072
num_hidden_layers: 12
num_attention_heads: 12
num_key_value_heads: 12
hidden_act: "silu"
initializer_range: 0.02
rms_norm_eps: 0.000001
use_cache: False
pad_token_id: 0
bos_token_id: 1
eos_token_id: 2
pretraining_tp: 1
tie_word_embeddings: False
rope_theta: 10000.0
attention_bias: False
attention_dropout: 0.0
mlp_bias: False
output_hidden_states: False
output_attentions: False